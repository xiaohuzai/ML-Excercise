# 基于决策树对连续值数据进行分类和回归

## 基于决策树分类

### 1、import需要的工具库

```python
# 用于数据处理和分析的工具包
import pandas as pd
# 引入用于数据预处理/特征工程的工具包
from sklearn import preprocessing
#引入决策树建模包
from sklearn import tree
```

### 2、加载数据

使用的是sklearn自带的鸢尾花数据

```python
from sklearn.datasets import load_iris
iris = load_iris()
# iris为一个字典，dir()返回iris的所有key值
dir(iris)
Out[3]: ['DESCR', 'data', 'feature_names', 'target', 'target_names']
```

看看iris数据

```python
iris_feature_name = iris.feature_names
iris_features = iris.data
iris_target_name = iris.target_names
iris_target = iris.target

iris_feature_name
Out[5]: 
['sepal length (cm)',
 'sepal width (cm)',
 'petal length (cm)',
 'petal width (cm)']

iris_features[:5,:]
Out[6]: 
array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
       [5. , 3.6, 1.4, 0.2]])

iris_target_name
Out[7]: array(['setosa', 'versicolor', 'virginica'], dtype='<U10')

iris_target
Out[8]: 
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

iris_features.shape
Out[9]: (150, 4)
```

可以看出iris的类别有三种：'setosa', 'versicolor', 'virginica'，总共有150条记录数据，用于分类的属性值有'sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'四种，都属于离散数据。

### 3、构建模型

```python
#规定决策树最大层为4层
clf = tree.DecisionTreeClassifier(max_depth=4)
clf = clf.fit(iris_features, iris_target)

clf
Out[11]: 
DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
```

### 4、作图

```python
# 输出dot格式的树状图，在这个里面我们将outfile设为None，也可以out_file='tree.dot'。然后可以转换成png格式：$ dot -Tpng tree.dot -o tree.png
dot_data = tree.export_graphviz(clf,
                                out_file = None,
                                feature_names = iris_feature_name,
                                class_names = iris_target_name,
                                filled=True,
                                rounded=True
                               )

#用pyplotplus画图
import pydotplus
from IPython.display import Image, display
graph = pydotplus.graph_from_dot_data(dot_data)
display(Image(graph.create_png()))

#另外一种方法是用graphviz
import graphviz 
graph = graphviz.Source(dot_data) 
graph
```

![](C:\ML-Excercise\pictures\decisiontree\1.png)

注：与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性。所以该图上第二层还会对petal width进行划分。

同样我们可以把个属性中两两配对，用两个属性对数据集进行分类

```python
import numpy as np
import matplotlib.pyplot as plt
# Parameters
n_classes = 3
plot_colors = "ryb"
plot_step = 0.02

#分别使用两两属性对原数据集分类
for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],[1, 2], [1, 3], [2, 3]]):
    # We only take the two corresponding features
    X = iris.data[:, pair]
    y = iris.target

    # Train
    clf = tree.DecisionTreeClassifier().fit(X, y)

    # Plot the decision boundary
    plt.subplot(2, 3, pairidx + 1)

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    #将整个区域画上格子点
    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                         np.arange(y_min, y_max, plot_step))
    #控制画图的边距
    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)
    
    #对整个区域进行预测
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    #画等高线并对区域填色，红黄蓝
    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)

    plt.xlabel(iris.feature_names[pair[0]])
    plt.ylabel(iris.feature_names[pair[1]])

    # Plot the training points
    for i, color in zip(range(n_classes), plot_colors):
        idx = np.where(y == i)
        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],
                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)

plt.suptitle("Decision surface of a decision tree using paired features")
plt.legend(loc='lower right', borderpad=0, handletextpad=0)
plt.axis("tight")
plt.show()
```

![](C:\ML-Excercise\pictures\decisiontree\2.png)

## 基于决策树和随机森林回归

### 1、加载数据

这个例子使用的是boston房价数据，加载和显示跟上面的鸢尾花数据差不多一样的操作。

```python
import pandas as pd
from sklearn import preprocessing
from sklearn import tree
from sklearn.datasets import load_boston
boston_house = load_boston()
boston_feature_name = boston_house.feature_names
boston_features = boston_house.data
boston_target = boston_house.target

#属性的名称
boston_feature_name
Out[19]: 
array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',
       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')
```

### 2、模型构建（决策树回归）

```python
#采用决策树回归模型
rgs = tree.DecisionTreeRegressor(max_depth=4)
rgs = rgs.fit(boston_features, boston_target)

rgs
Out[22]: 
DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,
           max_leaf_nodes=None, min_impurity_decrease=0.0,
           min_impurity_split=None, min_samples_leaf=1,
           min_samples_split=2, min_weight_fraction_leaf=0.0,
           presort=False, random_state=None, splitter='best')
```

### 3、画图

```python
import pydotplus
from IPython.display import Image, display
dot_data = tree.export_graphviz(rgs,
                                out_file = None,
                                feature_names = boston_feature_name,
                                class_names = boston_target,
                                filled = True,
                                rounded = True
                               )
graph = pydotplus.graph_from_dot_data(dot_data)
display(Image(graph.create_png()))
```

![](C:\ML-Excercise\pictures\decisiontree\3.png)

注：决策树回归采用的是递归二分方法，每一次划分都是二分，上图也可以看出。

### 4、采用随机森林回归

上面是采用的决策树回归模型，下面使用随机森林来对数据进行回归。

随机森林（Random Forest）相对于决策树，增加了两部分的随机过程，使得模型的泛化能力更强：

1、对训练集进行t次**随机采样**，例如可使用自主采样法（bootstrap sampling），每次有放回的采样m次，得到采样集Dm；

2、用Dm训练第m个决策树模型Gm(x)，在训练决策树结点的时候，在结点上所有属性中**随机选择**一部分属性，用这部分属性进行最优属性划分；

3、待t个决策树模型都训练完毕，对于分类问题，可以用t个决策树模型都预测一遍，取值多的为预测结果；对于回归问题，则是把t个模型的预测结果取平均值。

下面看看代码

```python
from sklearn.ensemble import RandomForestRegressor
#构建模型
rgs = RandomForestRegressor(n_estimators=15)
rgs = rgs.fit(boston_features, boston_target)

rgs
Out[40]:
RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
           max_features='auto', max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,
           oob_score=False, random_state=None, verbose=0, warm_start=False)
```

看看使用随机森林对波士顿房价的预测结果：

```python
rgs.predict(boston_features)
Out[41]:
array([25.81333333, 21.34666667, 35.20666667, 33.28666667, 34.48666667,
       27.41333333, 21.39333333, 22.98666667, 17.88      , 19.19333333,
       15.91333333, 20.12      , 20.99333333, 20.12      , 19.20666667,
       20.02      , 21.94      , 17.82666667, 19.15333333, 18.96      ,
       13.92666667, 18.99333333, 15.48666667, 16.44666667, 16.49333333,
       14.82      , 16.81333333, 15.30666667, 18.9       , 21.15333333,
       13.98666667, 17.64      , 15.28666667, 13.35333333, 13.62666667,
       20.22      , 20.03333333, 21.36      , 22.64666667, 29.71333333,
       33.46      , 27.83333333, 25.06666667, 25.17333333, 22.16666667,
       20.45333333, 20.6       , 16.55333333, 15.39333333, 19.97333333,
       19.50666667, 20.78      , 25.00666667, 22.57333333, 18.90666667,
       35.08      , 23.93333333, 31.65333333, 23.95333333, 19.66666667,
       18.76      , 16.43333333, 24.38666667, 25.01333333, 32.72666667,
       24.79333333, 19.85333333, 21.12      , 17.70666667, 20.84666667,
       24.18666667, 21.65333333, 22.96666667, 23.47333333, 24.38      ,
       20.96      , 20.45333333, 20.66666667, 20.97333333, 20.48666667,
       27.74666667, 25.35333333, 24.71333333, 23.34      , 24.10666667,
       26.72666667, 21.54666667, 21.73333333, 25.42666667, 29.13333333,
       22.62666667, 22.09333333, 22.45333333, 24.82666667, 20.52      ,
       28.11333333, 21.3       , 40.81333333, 43.11333333, 33.06      ,
       27.04      , 26.00666667, 20.25333333, 19.36      , 20.24666667,
       19.26666667, 18.88      , 20.41333333, 20.12666667, 19.31333333,
       21.13333333, 23.5       , 19.4       , 18.52666667, 19.94      ,
       18.59333333, 20.99333333, 20.30666667, 19.86      , 19.39333333,
       21.64666667, 20.90666667, 19.56      , 17.61333333, 18.76      ,
       19.64      , 16.26      , 16.        , 17.62      , 14.66666667,
       19.82      , 20.06666667, 21.81333333, 17.82666667, 15.15333333,
       17.46      , 16.33333333, 17.50666667, 14.16666667, 17.02      ,
       14.70666667, 14.81333333, 13.72      , 15.36      , 12.42666667,
       14.14666667, 16.27333333, 14.15333333, 16.46      , 14.94666667,
       20.69333333, 19.1       , 16.79333333, 18.32      , 17.52      ,
       15.26666667, 13.68666667, 37.15333333, 24.90666667, 25.35333333,
       26.57333333, 48.64666667, 50.        , 50.        , 22.70666667,
       24.92      , 49.36      , 22.61333333, 23.72666667, 22.51333333,
       17.37333333, 20.89333333, 21.09333333, 23.36666667, 21.47333333,
       28.97333333, 22.98666667, 24.83333333, 29.        , 34.        ,
       39.98      , 29.16666667, 35.97333333, 31.27333333, 25.06      ,
       29.29333333, 43.16666667, 29.76666667, 29.41333333, 34.68      ,
       35.42      , 29.64      , 35.65333333, 30.43333333, 29.06666667,
       49.38666667, 33.35333333, 30.46      , 33.11333333, 33.58      ,
       33.2       , 23.30666667, 43.02      , 48.75333333, 48.61333333,
       22.25333333, 23.11333333, 21.63333333, 22.95333333, 19.93333333,
       19.86666667, 19.32      , 22.21333333, 26.42666667, 21.53333333,
       23.89333333, 22.91333333, 27.11333333, 20.65333333, 22.87333333,
       27.76666667, 20.14666667, 26.58      , 29.75333333, 44.73333333,
       44.28666667, 40.01333333, 31.6       , 46.50666667, 31.34666667,
       23.28666667, 32.01333333, 41.63333333, 46.25333333, 27.91333333,
       23.37333333, 25.3       , 32.98      , 24.00666667, 24.52666667,
       22.86      , 20.41333333, 21.88666667, 23.94      , 18.18      ,
       19.02666667, 23.36666667, 20.42666667, 23.82666667, 25.9       ,
       24.34      , 25.75333333, 30.20666667, 41.69333333, 22.5       ,
       20.72      , 44.3       , 47.84      , 36.86      , 29.66      ,
       33.72666667, 44.36666667, 47.46      , 30.82      , 36.8       ,
       20.6       , 27.12666667, 47.72      , 44.77333333, 21.03333333,
       21.58666667, 25.49333333, 24.45333333, 36.39333333, 31.58666667,
       31.8       , 34.        , 32.24666667, 27.84666667, 33.72      ,
       45.24666667, 34.52666667, 45.06666667, 47.37333333, 31.06666667,
       21.90666667, 20.28      , 23.34      , 22.86666667, 24.62      ,
       29.70666667, 34.29333333, 28.24666667, 23.38      , 21.76666667,
       27.96      , 26.52      , 20.72      , 23.08      , 31.18666667,
       25.58      , 23.11333333, 25.46666667, 32.83333333, 34.2       ,
       27.1       , 33.59333333, 27.97333333, 24.84      , 20.25333333,
       17.14666667, 21.80666667, 19.48      , 22.3       , 23.5       ,
       17.81333333, 18.56666667, 20.36666667, 22.59333333, 21.47333333,
       23.74666667, 23.3       , 21.36      , 19.77333333, 24.42      ,
       24.82666667, 23.36666667, 20.64666667, 19.98      , 22.83333333,
       20.5       , 18.58666667, 20.04666667, 22.        , 21.33333333,
       20.98666667, 19.36666667, 18.67333333, 20.43333333, 19.29333333,
       19.02666667, 33.14666667, 17.36666667, 24.17333333, 30.59333333,
       17.91333333, 17.30666667, 23.35333333, 24.99333333, 26.49333333,
       23.47333333, 25.67333333, 20.12666667, 29.59333333, 17.61333333,
       20.17333333, 17.38666667, 21.54666667, 21.94666667, 22.06666667,
       23.82      , 19.68666667, 20.41333333, 17.15333333, 32.24      ,
       27.18666667, 20.15333333, 21.94      , 42.68      , 40.63333333,
       42.67333333, 46.73333333, 48.52666667, 13.57333333, 13.74      ,
       24.06      , 12.71333333, 14.41333333, 11.74      , 10.9       ,
       12.59333333, 11.32      , 11.64      , 11.76666667,  8.71333333,
        8.42      , 10.10666667,  8.16666667,  9.54      , 11.24      ,
       14.71333333, 20.21333333, 10.24      , 14.98666667, 13.88666667,
       13.61333333, 15.28      , 10.49333333,  6.75333333,  7.54666667,
        6.72666667,  7.88666667, 11.96      ,  9.70666667,  8.03333333,
        8.36666667, 13.01333333, 29.14666667, 16.20666667, 24.37333333,
       18.4       , 16.72      , 17.34666667, 16.02666667,  7.14666667,
        7.92666667,  9.12      ,  9.16      ,  8.47333333, 14.00666667,
       15.91333333, 15.36      , 20.32666667, 13.18      , 13.03333333,
        8.42      , 12.63333333, 13.52666667, 12.08      , 10.66666667,
       13.68666667, 20.34      , 19.56666667, 14.71333333, 11.65333333,
       13.06      , 10.92      ,  9.08      ,  8.38      , 12.44666667,
       10.39333333, 16.57333333, 17.85333333, 14.56666667, 10.98666667,
       11.94666667, 14.88      , 15.15333333, 14.08      , 13.80666667,
       13.34666667, 15.34666667, 17.34      , 21.28      , 14.6       ,
       14.63333333, 13.02      , 14.24666667, 14.73333333, 19.28666667,
       16.18      , 18.49333333, 19.81333333, 21.50666667, 21.21333333,
       19.83333333, 15.9       , 17.18666667, 17.34666667, 18.7       ,
       19.4       , 20.02      , 21.04      , 28.51333333, 13.94666667,
       14.44666667, 16.90666667, 13.16      , 13.92666667, 21.33333333,
       22.19333333, 23.79333333, 27.52      , 20.54      , 21.00666667,
       21.06      , 19.64666667, 20.66      , 15.49333333,  9.43333333,
        8.8       , 13.55333333, 20.60666667, 21.30666667, 22.90666667,
       21.24666667, 20.3       , 18.42666667, 20.3       , 19.11333333,
       17.76666667, 23.63333333, 19.62666667, 24.42666667, 23.24666667,
       14.38666667])
```

